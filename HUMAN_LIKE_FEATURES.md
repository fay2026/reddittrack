# ğŸ•µï¸ Human-Like Behavior Features

Your Reddit tracker now behaves like a real human to avoid detection and rate limiting!

## âœ¨ What Was Changed

### 1. **Random Delays Between Requests** â±ï¸
- Waits 2-5 seconds between actions (like a human reading)
- Longer breaks (3-8 seconds) between different subreddits
- Small pauses every 10 posts (simulating scrolling)

### 2. **Mixed Browsing Patterns** ğŸ”€
- Randomly browses "new", "hot", and "rising" posts
- Switches between different sorting methods (like using tabs)
- Randomizes the order of subreddit checks

### 3. **Varied Request Amounts** ğŸ“Š
- Slightly varies the number of posts fetched each time (Â±10)
- Not always fetching the exact same amount (more natural)
- Keeps limits reasonable (20-150 posts)

### 4. **Realistic User Agent** ğŸŒ
- Uses browser-like user agent strings
- Varies platform and version slightly
- Looks like: "MacOS:RedditTracker/1.0:v93.0 (by /u/tracker_bot)"

### 5. **Smart Error Handling** ğŸ›¡ï¸
- Detects rate limiting (HTTP 429)
- Automatically waits 60 seconds if rate limited
- Continues gracefully on errors

### 6. **Duplicate Removal** ğŸ”
- Removes posts that appear in multiple subreddits
- Prevents looking at the same content twice

## ğŸ“ˆ How It Works

### Before (Robotic):
```
â†’ Fetch 100 posts from r/complaints (new)
â†’ Fetch 100 posts from r/techsupport (new)
â†’ Fetch 100 posts from r/ProductComplaints (new)
â†’ Done in 5 seconds
```

### After (Human-like):
```
â†’ Shuffle subreddit order
â†’ Fetch 95 posts from r/techsupport (hot & rising)
   â±ï¸ Pause 0.5s every 10 posts
â†’ Wait 5.3 seconds (switching tabs)
â†’ Fetch 108 posts from r/complaints (new & hot)
   â±ï¸ Pause 1.2s every 10 posts
â†’ Wait 3.7 seconds (switching tabs)
â†’ Fetch 102 posts from r/ProductComplaints (rising & new)
â†’ Remove 3 duplicate posts
â†’ Done in 45 seconds
```

## âš™ï¸ Timing Breakdown

| Action | Delay | Purpose |
|--------|-------|---------|
| Between API calls | 2-5 seconds | Simulate reading posts |
| Every 10 posts | 0.5-2 seconds | Simulate scrolling |
| Between subreddits | 3-8 seconds | Simulate tab switching |
| After rate limit | 60 seconds | Comply with Reddit limits |

## ğŸ¯ Benefits

### âœ… Avoids Detection
- Looks like normal browsing activity
- Respects Reddit's rate limits
- Less likely to trigger bot detection

### âœ… Better Success Rate
- Automatically handles rate limiting
- Continues even if one subreddit fails
- More reliable long-term

### âœ… More Natural Data
- Gets posts from multiple sorting methods
- Catches trending posts you might miss
- Better coverage of complaints/demands

## ğŸ”§ Customization

You can adjust the timing in `reddit_fetcher.py`:

### Make it Slower (More Cautious)
```python
self._human_delay(5, 10)  # Wait 5-10 seconds instead of 2-5
```

### Make it Faster (More Aggressive)
```python
self._human_delay(1, 2)  # Wait 1-2 seconds
```

**Note**: Being too fast may trigger rate limits!

## ğŸ“Š Expected Performance

With human-like behavior:
- **Time per subreddit**: ~15-30 seconds
- **3 subreddits**: ~1-2 minutes total
- **Success rate**: Much higher
- **Detection risk**: Much lower

## ğŸ›¡ï¸ Rate Limit Protection

Reddit's official limits:
- **60 requests per minute** for authenticated apps
- **600 requests per 10 minutes**

Our approach:
- ~10-15 requests per minute (well under limit)
- Automatic 60s cooldown if limited
- Natural browsing patterns

## ğŸ’¡ Tips

1. **Don't check too frequently**: Once per day is ideal
2. **Monitor logs**: Watch for rate limit warnings
3. **Adjust delays**: If you get rate limited, increase delays
4. **Use fewer subreddits**: Start with 2-3, expand later
5. **Be patient**: Slower = safer and more reliable

## ğŸ­ The Result

Your tracker now looks like:
- Someone browsing Reddit casually
- Checking different tabs and sorting options
- Reading posts at a natural pace
- Taking breaks between sections

**Not like**:
- A bot hitting the API rapidly
- Automated script with fixed timing
- Mass data scraper

---

**Stay under the radar! ğŸ¥·**
